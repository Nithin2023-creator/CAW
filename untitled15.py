# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11TQBa7kbDvqxsnHBJ2xl46NyKXRw8E0E
"""

!pip install streamlit langchain langchain-community langchain-google-genai langchain-pinecone pinecone-client google-generativeai PyPDF2 python-docx markdown beautifulsoup4 unstructured

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_google_genai import GoogleGenerativeAIEmbeddings
# from langchain_pinecone import Pinecone as PineconeVectorStore
# from pinecone import Pinecone, ServerlessSpec
# import os
# from pathlib import Path
# import json
# from datetime import datetime
# import google.generativeai as genai
# import uuid
# import re
# 
# # Streamlit app configuration
# st.set_page_config(page_title="Document Processing Pipeline", layout="wide")
# st.title("Document Processing Pipeline")
# st.subheader("Upload and process documents (PDF, DOCX, TXT, HTML, Markdown)")
# 
# # Sidebar for configuration
# st.sidebar.header("Configuration")
# chunk_size = st.sidebar.slider("Chunk Size (characters)", 500, 2000, 1000, 100)
# chunk_overlap = st.sidebar.slider("Chunk Overlap (characters)", 0, 500, 200, 50)
# chunking_strategy = st.sidebar.selectbox("Chunking Strategy", ["Semantic", "Sliding Window", "Dynamic"])
# 
# # Hardcoded API keys
# gemini_api_key = "AIzaSyB3sZS0oKnuVSyjyZKxyvRjpKa7nIoKYGg"
# pinecone_api_key = "pcsk_2samF1_Gu8hGM5YMPrZqqmGvADSYUvVE47EYJFFBJz5w5mZ3QinynS2Q7yagqcTPDQphtS"
# pinecone_index_name = "document-processing1"
# 
# # Initialize session state for user ID
# if "user_id" not in st.session_state:
#     st.session_state.user_id = str(uuid.uuid4())
# 
# # Initialize chat history in session state
# if 'messages' not in st.session_state:
#     st.session_state.messages = []
# 
# # Initialize feedback states
# if 'feedback_states' not in st.session_state:
#     st.session_state.feedback_states = {}
# 
# # Initialize Gemini embeddings
# try:
#     genai.configure(api_key=gemini_api_key)
#     embeddings_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=gemini_api_key)
# except Exception as e:
#     embeddings_model = None
#     st.error(f"Error initializing Gemini embeddings: {str(e)}")
# 
# # Initialize Pinecone
# try:
#     os.environ["PINECONE_API_KEY"] = pinecone_api_key
#     pc = Pinecone(api_key=pinecone_api_key)
#     if pinecone_index_name not in pc.list_indexes().names():
#         pc.create_index(
#             name=pinecone_index_name,
#             dimension=768,
#             metric="cosine",
#             spec=ServerlessSpec(cloud="aws", region="us-east-1")
#         )
#     vector_store = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings_model, namespace="chunks")
# except Exception as e:
#     st.error(f"Error initializing Pinecone: {str(e)}")
#     vector_store = None
# 
# # Memory Management System
# class MemoryManager:
#     def __init__(self, pc, index_name):
#         self.pc = pc
#         self.index_name = index_name
#         self.short_term_memory = []  # Last 20 exchanges
#         self.max_short_term_size = 20
#         self.conversation_context = {
#             'topics': {},
#             'current_context': '',
#             'last_exchange': None
#         }
# 
#     def add_to_short_term_memory(self, query, response):
#         # Create a new exchange
#         exchange = {
#             'query': query,
#             'response': response,
#             'timestamp': datetime.now().isoformat(),
#             'topics': self._extract_topics(query, response)
#         }
# 
#         # Update short term memory
#         self.short_term_memory.append(exchange)
#         if len(self.short_term_memory) > self.max_short_term_size:
#             self.short_term_memory.pop(0)
# 
#         # Update conversation context
#         self._update_conversation_context(exchange)
# 
#     def _extract_topics(self, query, response):
#         try:
#             model = genai.GenerativeModel('gemini-1.5-flash')
#             prompt = f"""Extract 3-5 key topics from this conversation exchange.
#             Format: Return ONLY a comma-separated list of topics, no other text.
# 
#             User: {query}
#             Assistant: {response}"""
# 
#             response = model.generate_content(prompt)
#             # Clean and validate the response
#             topics_text = response.text.strip()
#             if topics_text.startswith('['):
#                 topics_text = topics_text[1:]
#             if topics_text.endswith(']'):
#                 topics_text = topics_text[:-1]
#             topics = [topic.strip().strip('"\'') for topic in topics_text.split(',')]
#             return topics
#         except Exception as e:
#             st.error(f"Error extracting topics: {str(e)}")
#             return []
# 
#     def _update_conversation_context(self, exchange):
#         try:
#             model = genai.GenerativeModel('gemini-1.5-flash')
# 
#             # Prepare the current context summary
#             current_context = self.conversation_context['current_context']
# 
#             prompt = f"""Analyze this exchange and update the conversation context.
#             Current conversation context: {current_context}
# 
#             New exchange:
#             User: {exchange['query']}
#             Assistant: {exchange['response']}
# 
#             Instructions:
#             1. Identify key topics discussed
#             2. Note any references to previous topics
#             3. Summarize the current state of the conversation
# 
#             Format your response in exactly this structure (keep the exact keys):
#             {{
#                 "topics_discussed": ["topic1", "topic2"],
#                 "references_to_previous": ["reference1", "reference2"],
#                 "conversation_summary": "A brief summary of the current state"
#             }}"""
# 
#             response = model.generate_content(prompt)
# 
#             # Ensure proper JSON formatting
#             try:
#                 # Clean the response text to ensure it's valid JSON
#                 response_text = response.text.strip()
#                 if response_text.startswith('```json'):
#                     response_text = response_text[7:]
#                 if response_text.endswith('```'):
#                     response_text = response_text[:-3]
#                 response_text = response_text.strip()
# 
#                 context_update = json.loads(response_text)
# 
#                 # Update topics
#                 for topic in context_update.get('topics_discussed', []):
#                     if topic not in self.conversation_context['topics']:
#                         self.conversation_context['topics'][topic] = {
#                             'first_mentioned': len(self.short_term_memory) - 1,
#                             'mentions': [],
#                             'latest_context': ''
#                         }
#                     self.conversation_context['topics'][topic]['mentions'].append(len(self.short_term_memory) - 1)
#                     self.conversation_context['topics'][topic]['latest_context'] = context_update['conversation_summary']
# 
#                 # Update current context
#                 self.conversation_context['current_context'] = context_update['conversation_summary']
#                 self.conversation_context['last_exchange'] = exchange
# 
#             except json.JSONDecodeError as e:
#                 st.error(f"Error parsing context update: {str(e)}")
#                 # Fallback: Store basic context
#                 self.conversation_context['current_context'] = f"Last query: {exchange['query']}"
#                 self.conversation_context['last_exchange'] = exchange
# 
#         except Exception as e:
#             st.error(f"Error updating conversation context: {str(e)}")
# 
#     def get_relevant_context(self, query, include_last_n=3):
#         try:
#             # Get recent context
#             recent_context = self.short_term_memory[-include_last_n:] if self.short_term_memory else []
# 
#             # Get current topics from the query
#             query_topics = self._extract_topics(query, "")
# 
#             # Build context information
#             context = {
#                 'recent_exchanges': recent_context,
#                 'current_context': self.conversation_context['current_context'],
#                 'related_topics': {},
#                 'conversation_summary': []
#             }
# 
#             # Add topic-specific context
#             for topic in query_topics:
#                 if topic in self.conversation_context['topics']:
#                     topic_data = self.conversation_context['topics'][topic]
#                     context['related_topics'][topic] = {
#                         'latest_context': topic_data['latest_context'],
#                         'related_exchanges': [
#                             self.short_term_memory[idx]
#                             for idx in topic_data['mentions'][-3:]
#                             if idx < len(self.short_term_memory)
#                         ]
#                     }
# 
#             # Add conversation summaries
#             if recent_context:
#                 context['conversation_summary'] = [
#                     f"User: {ex['query']}\nAssistant: {ex['response']}"
#                     for ex in recent_context
#                 ]
# 
#             return context
# 
#         except Exception as e:
#             st.error(f"Error getting relevant context: {str(e)}")
#             return {
#                 'recent_exchanges': [],
#                 'current_context': '',
#                 'related_topics': {},
#                 'conversation_summary': []
#             }
# 
#     def store_long_term_memory(self, query, response, topic, success_score):
#         try:
#             if not self.pc:
#                 return
# 
#             memory_id = f"ltm_{uuid.uuid4()}"
#             memory_embedding = embeddings_model.embed_query(f"{query} {response}")
# 
#             self.pc.Index(self.index_name).upsert(
#                 vectors=[{
#                     'id': memory_id,
#                     'values': memory_embedding,
#                     'metadata': {
#                         'query': query,
#                         'response': response,
#                         'topic': topic,
#                         'success_score': success_score,
#                         'timestamp': datetime.now().isoformat(),
#                         'type': 'long_term_memory'
#                     }
#                 }],
#                 namespace="memories"
#             )
#         except Exception as e:
#             st.error(f"Error storing long-term memory: {str(e)}")
# 
#     def store_episodic_memory(self, session_id, interactions):
#         try:
#             if not self.pc:
#                 return
# 
#             episode_id = f"episode_{session_id}_{datetime.now().isoformat()}"
#             episode_summary = self._generate_episode_summary(interactions)
#             episode_embedding = embeddings_model.embed_query(episode_summary)
# 
#             self.pc.Index(self.index_name).upsert(
#                 vectors=[{
#                     'id': episode_id,
#                     'values': episode_embedding,
#                     'metadata': {
#                         'session_id': session_id,
#                         'summary': episode_summary,
#                         'interactions': interactions,
#                         'timestamp': datetime.now().isoformat(),
#                         'type': 'episodic_memory'
#                     }
#                 }],
#                 namespace="memories"
#             )
#         except Exception as e:
#             st.error(f"Error storing episodic memory: {str(e)}")
# 
#     def _generate_episode_summary(self, interactions):
#         try:
#             model = genai.GenerativeModel('gemini-1.5-flash')
#             interactions_text = "\n".join([f"Q: {i['query']}\nA: {i['response']}" for i in interactions])
#             prompt = f"""Summarize this conversation session concisely:
# 
#             {interactions_text}
# 
#             Generate a brief summary that captures the main topics and outcomes."""
# 
#             response = model.generate_content(prompt)
#             return response.text
#         except Exception as e:
#             st.error(f"Error generating episode summary: {str(e)}")
#             return "Error generating summary"
# 
#     def get_relevant_memories(self, query, top_k=3):
#         try:
#             if not self.pc:
#                 return []
# 
#             query_embedding = embeddings_model.embed_query(query)
#             results = self.pc.Index(self.index_name).query(
#                 namespace="memories",
#                 vector=query_embedding,
#                 top_k=top_k,
#                 include_metadata=True
#             )
# 
#             return [match['metadata'] for match in results['matches']]
#         except Exception as e:
#             st.error(f"Error retrieving memories: {str(e)}")
#             return []
# 
# # Initialize memory manager
# if 'memory_manager' not in st.session_state:
#     st.session_state.memory_manager = MemoryManager(pc, pinecone_index_name)
# 
# # Initialize session state for conversation history
# if 'conversation_history' not in st.session_state:
#     st.session_state.conversation_history = []
# 
# # Function to load document based on file type
# def load_document(file_path, file_type):
#     try:
#         if file_type == "pdf":
#             loader = PyPDFLoader(file_path)
#         elif file_type == "docx":
#             loader = Docx2txtLoader(file_path)
#         elif file_type == "txt":
#             loader = TextLoader(file_path)
#         elif file_type == "html":
#             loader = UnstructuredHTMLLoader(file_path)
#         elif file_type == "md":
#             loader = UnstructuredMarkdownLoader(file_path)
#         else:
#             st.error("Unsupported file type")
#             return None
#         return loader.load()
#     except Exception as e:
#         st.error(f"Error loading document: {str(e)}")
#         return None
# 
# # Semantic chunking based on paragraph boundaries
# def semantic_chunking(docs, chunk_size, chunk_overlap):
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=chunk_size,
#         chunk_overlap=chunk_overlap,
#         separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
#     )
#     return text_splitter.split_documents(docs)
# 
# # Sliding window chunking
# def sliding_window_chunking(docs, chunk_size, chunk_overlap):
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=chunk_size,
#         chunk_overlap=chunk_overlap
#     )
#     return text_splitter.split_documents(docs)
# 
# # Dynamic chunk sizing based on content type
# def dynamic_chunking(docs, file_type):
#     if file_type == "pdf":
#         chunk_size, chunk_overlap = 800, 150
#     elif file_type == "txt":
#         chunk_size, chunk_overlap = 1200, 300
#     elif file_type == "docx":
#         chunk_size, chunk_overlap = 1000, 200
#     elif file_type == "html" or file_type == "md":
#         chunk_size, chunk_overlap = 600, 100
#     else:
#         chunk_size, chunk_overlap = 1000, 200
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=chunk_size,
#         chunk_overlap=chunk_overlap
#     )
#     return text_splitter.split_documents(docs)
# 
# # Section detection (simplified: based on headings or double newlines)
# def detect_sections(docs):
#     sections = []
#     current_section = ""
#     for doc in docs:
#         content = doc.page_content
#         lines = content.split("\n")
#         for line in lines:
#             if line.startswith(("#", "##", "###")) or re.match(r"<h[1-6]>", line):
#                 if current_section:
#                     sections.append(current_section.strip())
#                     current_section = ""
#             current_section += line + "\n"
#         if current_section:
#             sections.append(current_section.strip())
#             current_section = ""
#     return sections if sections else [doc.page_content for doc in docs]
# 
# # Generate hierarchical embeddings
# def generate_embeddings(docs, sections, chunks, file_name, file_type):
#     embeddings_data = {"document": {}, "sections": [], "chunks": []}
#     timestamp = datetime.now().isoformat()
# 
#     # Document-level embedding
#     if embeddings_model:
#         try:
#             full_text = " ".join([doc.page_content for doc in docs])
#             doc_embedding = embeddings_model.embed_query(full_text[:10000])  # Limit text length for document level
#             embeddings_data["document"] = {
#                 "embedding": doc_embedding,
#                 "metadata": {
#                     "source": file_name,
#                     "timestamp": timestamp,
#                     "type": "document"
#                 }
#             }
#         except Exception as e:
#             st.error(f"Error generating document embedding: {str(e)}")
# 
#     # Section-level embeddings
#     if embeddings_model:
#         for i, section in enumerate(sections):
#             try:
#                 # Limit section text length
#                 section_text = section[:5000]
#                 section_embedding = embeddings_model.embed_query(section_text)
#                 embeddings_data["sections"].append({
#                     "embedding": section_embedding,
#                     "metadata": {
#                         "source": file_name,
#                         "section_id": i + 1,
#                         "timestamp": timestamp,
#                         "type": "section",
#                         "content": section  # Store full section content
#                     }
#                 })
#             except Exception as e:
#                 st.error(f"Error generating section {i+1} embedding: {str(e)}")
# 
#     # Chunk-level embeddings with progress tracking
#     if embeddings_model:
#         with st.spinner('Generating chunk embeddings...'):
#             progress_bar = st.progress(0)
#             total_chunks = len(chunks)
# 
#             for i, chunk in enumerate(chunks):
#                 try:
#                     # Update progress
#                     progress = int((i + 1) / total_chunks * 100)
#                     progress_bar.progress(progress)
# 
#                     # Generate embedding for chunk
#                     chunk_text = chunk.page_content
#                     if chunk_text.strip():  # Only process non-empty chunks
#                         chunk_embedding = embeddings_model.embed_query(chunk_text)
#                         metadata = {
#                             "source": file_name,
#                             "chunk_id": i + 1,
#                             "timestamp": timestamp,
#                             "type": "chunk",
#                             "content": chunk_text
#                         }
#                         if file_type == "pdf" and hasattr(chunk, "metadata") and "page" in chunk.metadata:
#                             metadata["page_number"] = chunk.metadata["page"]
# 
#                         embeddings_data["chunks"].append({
#                             "embedding": chunk_embedding,
#                             "metadata": metadata
#                         })
#                 except Exception as e:
#                     st.error(f"Error generating chunk {i+1} embedding: {str(e)}")
# 
#             progress_bar.empty()  # Remove progress bar when done
# 
#     # Verify embeddings were generated
#     if not embeddings_data["chunks"]:
#         st.warning("No chunk embeddings were generated. Please check your document content.")
#     else:
#         st.success(f"Successfully generated {len(embeddings_data['chunks'])} chunk embeddings")
# 
#     return embeddings_data
# 
# # Store embeddings in Pinecone
# def store_in_pinecone(embeddings_data, pc, index_name):
#     try:
#         index = pc.Index(index_name)
#         doc_id = f"doc_{embeddings_data['document']['metadata']['source']}_{embeddings_data['document']['metadata']['timestamp']}"
#         index.upsert(
#             vectors=[{
#                 "id": doc_id,
#                 "values": embeddings_data["document"]["embedding"],
#                 "metadata": embeddings_data["document"]["metadata"]
#             }],
#             namespace="chunks"
#         )
#         for section in embeddings_data["sections"]:
#             section_id = f"section_{section['metadata']['source']}_{section['metadata']['section_id']}_{section['metadata']['timestamp']}"
#             index.upsert(
#                 vectors=[{
#                     "id": section_id,
#                     "values": section["embedding"],
#                     "metadata": section["metadata"]
#                 }],
#                 namespace="chunks"
#             )
#         for chunk in embeddings_data["chunks"]:
#             chunk_id = f"chunk_{chunk['metadata']['source']}_{chunk['metadata']['chunk_id']}_{chunk['metadata']['timestamp']}"
#             index.upsert(
#                 vectors=[{
#                     "id": chunk_id,
#                     "values": chunk["embedding"],
#                     "metadata": chunk["metadata"]
#                 }],
#                 namespace="chunks"
#             )
#         st.success("Embeddings stored in Pinecone successfully!")
#     except Exception as e:
#         st.error(f"Error storing embeddings in Pinecone: {str(e)}")
# 
# # Query Pinecone
# def query_pinecone(query, pc, index_name, top_k=3):
#     if not embeddings_model or not pc:
#         return None
#     try:
#         index = pc.Index(index_name)
#         query_embedding = embeddings_model.embed_query(query)
#         results = index.query(
#             namespace="chunks",
#             vector=query_embedding,
#             top_k=top_k,
#             include_metadata=True
#         )
#         return results
#     except Exception as e:
#         st.error(f"Error querying Pinecone: {str(e)}")
#         return None
# 
# # Function to get response from Gemini
# def get_gemini_response(query, retrieved_content):
#     try:
#         # Initialize the Gemini model
#         model = genai.GenerativeModel('gemini-1.5-flash')
# 
#         # Get the last query expansion details if available
#         last_expansion = st.session_state.memory_manager.conversation_context.get('last_expansion', {})
#         expansion_context = ""
#         if last_expansion:
#             expansion_context = f"""
# Original query: {last_expansion.get('original_query', '')}
# Expanded to: {last_expansion.get('expanded_query', '')}
# Based on context: {chr(10).join(last_expansion.get('context_used', []))}
# """
# 
#         # Prepare the prompt with query context and retrieved content
#         prompt = f"""Generate a response to the user's query using the provided context.
# 
# Query Context:
# {expansion_context}
# 
# Current Query: {query}
# 
# Retrieved Content:
# {retrieved_content}
# 
# Instructions:
# 1. Answer the query using information from the retrieved content
# 2. Maintain context from the conversation
# 3. If referring to previous topics, be explicit about what you're referring to
# 4. If the answer requires information from previous exchanges, include that context
# 5. Be clear and concise while being comprehensive
# 
# Generate a natural, conversational response that directly answers the query."""
# 
#         # Generate response
#         response = model.generate_content(prompt)
#         return response.text
#     except Exception as e:
#         st.error(f"Error generating response from Gemini: {str(e)}")
#         return None
# 
# # Store interaction in Pinecone
# def store_interaction(query, pc, index_name):
#     if not embeddings_model or not pc:
#         return
#     try:
#         index = pc.Index(index_name)
#         query_embedding = embeddings_model.embed_query(query)
#         interaction_id = f"interaction_{st.session_state.user_id}_{datetime.now().isoformat()}"
#         index.upsert(
#             vectors=[{
#                 "id": interaction_id,
#                 "values": query_embedding,
#                 "metadata": {
#                     "user_id": st.session_state.user_id,
#                     "query": query,
#                     "timestamp": datetime.now().isoformat(),
#                     "type": "interaction"
#                 }
#             }],
#             namespace="interactions"
#         )
#     except Exception as e:
#         st.error(f"Error storing interaction: {str(e)}")
# 
# # Store feedback in Pinecone
# def store_feedback(query_id, feedback, rating, pc, index_name):
#     if not pc:
#         return
#     try:
#         index = pc.Index(index_name)
#         feedback_id = f"feedback_{st.session_state.user_id}_{datetime.now().isoformat()}"
#         index.upsert(
#             vectors=[{
#                 "id": feedback_id,
#                 "values": [0.0] * 768,
#                 "metadata": {
#                     "user_id": st.session_state.user_id,
#                     "query_id": query_id,
#                     "feedback": feedback,
#                     "rating": rating,
#                     "timestamp": datetime.now().isoformat(),
#                     "type": "feedback"
#                 }
#             }],
#             namespace="feedback"
#         )
#     except Exception as e:
#         st.error(f"Error storing feedback: {str(e)}")
# 
# # Store query-answer pair in Pinecone
# def store_query_answer(query, answer, pc, index_name):
#     if not embeddings_model or not pc:
#         return
#     try:
#         index = pc.Index(index_name)
#         query_embedding = embeddings_model.embed_query(query)
#         qa_id = f"qa_{st.session_state.user_id}_{datetime.now().isoformat()}"
#         index.upsert(
#             vectors=[{
#                 "id": qa_id,
#                 "values": query_embedding,
#                 "metadata": {
#                     "query": query,
#                     "answer": answer,
#                     "timestamp": datetime.now().isoformat(),
#                     "type": "query_answer"
#                 }
#             }],
#             namespace="query_answer_pairs"
#         )
#     except Exception as e:
#         st.error(f"Error storing query-answer pair: {str(e)}")
# 
# # Query expansion and reformulation
# def expand_query(query, conversation_history, memory_manager):
#     try:
#         # Get relevant context from memory manager
#         context = memory_manager.get_relevant_context(query)
# 
#         model = genai.GenerativeModel('gemini-1.5-flash')
# 
#         # Build context sections
#         context_sections = []
# 
#         # Add current conversation context
#         if context['current_context']:
#             context_sections.append(f"Current conversation context:\n{context['current_context']}")
# 
#         # Add recent conversation summary
#         if context['conversation_summary']:
#             context_sections.append("Recent conversation:\n" + "\n".join(context['conversation_summary']))
# 
#         # Add topic-specific context
#         if context['related_topics']:
#             topic_sections = []
#             for topic, topic_data in context['related_topics'].items():
#                 topic_context = f"Topic: {topic}\n"
#                 topic_context += f"Context: {topic_data['latest_context']}\n"
#                 if topic_data['related_exchanges']:
#                     topic_context += "Related exchanges:\n"
#                     for exchange in topic_data['related_exchanges']:
#                         topic_context += f"Q: {exchange['query']}\nA: {exchange['response']}\n"
#                 topic_sections.append(topic_context)
#             context_sections.append("Topic-specific context:\n" + "\n".join(topic_sections))
# 
#         prompt = f"""Given the conversation context and current query, create an expanded search query.
# 
# Context:
# {chr(10).join(context_sections)}
# 
# Current Query: {query}
# 
# Instructions:
# 1. Analyze how the query relates to the conversation context
# 2. Identify any references to previous topics or information
# 3. Resolve pronouns and implicit references
# 4. Include relevant context from previous exchanges
# 5. Maintain the original intent of the question
# 
# Return ONLY the expanded query text, no explanations."""
# 
#         response = model.generate_content(prompt)
#         expanded_query = response.text.strip()
# 
#         # Log the expansion for debugging
#         st.session_state.memory_manager.conversation_context['last_expansion'] = {
#             'original_query': query,
#             'expanded_query': expanded_query,
#             'context_used': context_sections
#         }
# 
#         return expanded_query
#     except Exception as e:
#         st.error(f"Error expanding query: {str(e)}")
#         return query
# 
# def reformulate_query(query, user_profile):
#     try:
#         model = genai.GenerativeModel('gemini-1.5-flash')
#         prompt = f"""Given the user's profile and preferences, reformulate the query to be more precise:
# 
#         User Profile:
#         {json.dumps(user_profile, indent=2)}
# 
#         Original Query: {query}
# 
#         Generate a reformulated query that considers the user's preferences and past interactions."""
# 
#         response = model.generate_content(prompt)
#         reformulated_query = response.text
#         return reformulated_query
#     except Exception as e:
#         st.error(f"Error reformulating query: {str(e)}")
#         return query
# 
# # Enhanced query processing with hybrid search
# def hybrid_search(query, pc, index_name, top_k=5):
#     if not embeddings_model or not pc:
#         return None
#     try:
#         # Semantic search
#         semantic_results = query_pinecone(query, pc, index_name, top_k=top_k)
# 
#         # Keyword search using regex patterns
#         keyword_results = []
#         keywords = query.lower().split()
#         index = pc.Index(index_name)
# 
#         # Fetch recent vectors
#         recent_vectors = index.query(
#             namespace="chunks",
#             vector=[0.0] * 768,  # Dummy vector to fetch recent vectors
#             top_k=100,
#             include_metadata=True
#         )
# 
#         for vector in recent_vectors.matches:
#             content = vector.metadata.get('content', '').lower()
#             keyword_score = sum(1 for keyword in keywords if keyword in content)
#             if keyword_score > 0:
#                 keyword_results.append({
#                     'id': vector.id,
#                     'score': keyword_score / len(keywords),
#                     'metadata': vector.metadata
#                 })
# 
#         # Combine and re-rank results
#         combined_results = []
#         seen_ids = set()
# 
#         # Add semantic results
#         if semantic_results and 'matches' in semantic_results:
#             for match in semantic_results['matches']:
#                 combined_results.append({
#                     'id': match['id'],
#                     'score': match['score'] * 0.7,  # Weight for semantic score
#                     'metadata': match['metadata']
#                 })
#                 seen_ids.add(match['id'])
# 
#         # Add keyword results
#         for result in keyword_results:
#             if result['id'] not in seen_ids:
#                 combined_results.append({
#                     'id': result['id'],
#                     'score': result['score'] * 0.3,  # Weight for keyword score
#                     'metadata': result['metadata']
#                 })
# 
#         # Sort by combined score
#         combined_results.sort(key=lambda x: x['score'], reverse=True)
#         return {'matches': combined_results[:top_k]}
#     except Exception as e:
#         st.error(f"Error in hybrid search: {str(e)}")
#         return None
# 
# # File uploader for document upload
# uploaded_file = st.file_uploader("Upload a document (PDF, DOCX, TXT, HTML, Markdown)", type=["pdf", "docx", "txt", "html", "md"])
# 
# # Main processing logic
# if uploaded_file:
#     file_name = uploaded_file.name
#     file_type = file_name.split(".")[-1].lower()
#     file_path = f"temp.{file_type}"
#     with open(file_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
#     docs = load_document(file_path, file_type)
#     if docs:
#         st.success("Document loaded successfully!")
#         sections = detect_sections(docs)
#         st.subheader("Detected Sections")
#         st.write(f"Total sections: {len(sections)}")
#         for i, section in enumerate(sections[:3]):
#             with st.expander(f"Section {i+1}"):
#                 st.write(section[:500] + "..." if len(section) > 500 else section)
#         if chunking_strategy == "Semantic":
#             chunks = semantic_chunking(docs, chunk_size, chunk_overlap)
#         elif chunking_strategy == "Sliding Window":
#             chunks = sliding_window_chunking(docs, chunk_size, chunk_overlap)
#         elif chunking_strategy == "Dynamic":
#             chunks = dynamic_chunking(docs, file_type)
#         st.subheader("Processed Chunks")
#         st.write(f"Total chunks: {len(chunks)}")
#         for i, chunk in enumerate(chunks[:3]):
#             with st.expander(f"Chunk {i+1}"):
#                 st.write(chunk.page_content)
#         if embeddings_model and vector_store:
#             embeddings_data = generate_embeddings(docs, sections, chunks, file_name, file_type)
#             store_in_pinecone(embeddings_data, pc, pinecone_index_name)
#             st.write(f"Document-level embedding stored: {len(embeddings_data['document'].get('embedding', []))} dimensions")
#             st.write(f"Section-level embeddings stored: {len(embeddings_data['sections'])}")
#             st.write(f"Chunk-level embeddings stored: {len(embeddings_data['chunks'])}")
#     if os.path.exists(file_path):
#         os.remove(file_path)
# 
# # Chat interface
# st.subheader("Document Q&A Chatbot")
# 
# # Display chat messages
# for idx, message in enumerate(st.session_state.messages):
#     with st.chat_message(message["role"]):
#         st.write(message["content"])
#         if message["role"] == "assistant":
#             # Create feedback buttons
#             col1, col2, col3 = st.columns([1, 8, 1])
# 
#             with col1:
#                 if st.button("ðŸ‘", key=f"like_{idx}"):
#                     st.success("Thank you for your feedback!")
#                     st.rerun()
# 
#             with col3:
#                 if st.button("ðŸ‘Ž", key=f"dislike_{idx}"):
#                     # Show feedback form
#                     feedback = st.text_area("Please tell us what was wrong or missing:", key=f"feedback_{idx}")
#                     if st.button("Submit Feedback", key=f"submit_{idx}"):
#                         if feedback:
#                             # Generate improved response
#                             model = genai.GenerativeModel('gemini-1.5-flash')
#                             improvement_prompt = f"""
#                             Original response: {message['content']}
#                             User feedback: {feedback}
# 
#                             Please provide an improved response that addresses the feedback.
#                             If they asked for bullet points, use bullet points.
#                             If they asked for a shorter response, be more concise.
#                             If they asked for more details, be more comprehensive.
#                             """
# 
#                             improved_response = model.generate_content(improvement_prompt)
#                             improved_text = improved_response.text
# 
#                             # Add improved response to chat
#                             st.session_state.messages.append({
#                                 "role": "assistant",
#                                 "content": improved_text,
#                                 "is_improvement": True
#                             })
#                             st.rerun()
#                         else:
#                             st.warning("Please provide feedback before submitting.")
# 
# # Chat input
# if prompt := st.chat_input("Ask me anything about your documents"):
#     # Display user message
#     with st.chat_message("user"):
#         st.write(prompt)
#     st.session_state.messages.append({"role": "user", "content": prompt})
# 
#     # Display assistant response
#     with st.chat_message("assistant"):
#         if vector_store and embeddings_model:
#             # Get user profile from session state or initialize
#             if 'user_profile' not in st.session_state:
#                 st.session_state.user_profile = {
#                     'session_id': st.session_state.user_id,
#                     'preferences': {},
#                     'interaction_count': 0
#                 }
# 
#             # Process query with context
#             with st.spinner("Thinking..."):
#                 # Expand and reformulate query using conversation history
#                 expanded_query = expand_query(prompt, st.session_state.conversation_history, st.session_state.memory_manager)
#                 reformulated_query = reformulate_query(expanded_query, st.session_state.user_profile)
# 
#                 # Get relevant memories
#                 relevant_memories = st.session_state.memory_manager.get_relevant_memories(reformulated_query)
# 
#                 # Perform hybrid search
#                 search_results = hybrid_search(reformulated_query, pc, pinecone_index_name)
# 
#                 if search_results and "matches" in search_results:
#                     retrieved_contents = []
#                     context_sections = []
# 
#                     # Format relevant memories
#                     if relevant_memories:
#                         memory_context = "Previous relevant interactions:\n"
#                         for memory in relevant_memories:
#                             memory_context += f"Q: {memory.get('query', '')}\nA: {memory.get('response', '')}\n"
#                         context_sections.append(memory_context)
# 
#                     # Format search results
#                     for match in search_results["matches"]:
#                         content = match['metadata'].get('content', '')
#                         if content and content != 'N/A':
#                             source = match['metadata'].get('source', 'Unknown')
#                             page = match['metadata'].get('page_number', '')
#                             page_info = f" (Page {page})" if page else ""
#                             context_sections.append(f"From {source}{page_info}:\n{content}")
#                             retrieved_contents.append(content)
# 
#                     # Combine all context
#                     retrieved_text = "\n\n".join(context_sections)
# 
#                     if not retrieved_text:
#                         retrieved_text = "No relevant content found."
# 
#                     # Get response from Gemini
#                     response_message = get_gemini_response(prompt, retrieved_text)
#                     if response_message:
#                         # Display the response
#                         st.write(response_message)
# 
#                         # Immediate feedback UI
#                         feedback_col1, feedback_col2, feedback_col3 = st.columns([1, 8, 1])
# 
#                         # Show feedback buttons if feedback hasn't been given
#                         if "feedback_given" not in st.session_state:
#                             st.session_state.feedback_given = False
# 
#                         if not st.session_state.feedback_given:
#                             with feedback_col1:
#                                 if st.button("ðŸ‘", help="This response was helpful"):
#                                     st.session_state.feedback_given = True
#                                     st.session_state.memory_manager.store_long_term_memory(
#                                         query=prompt,
#                                         response=response_message,
#                                         topic="general",
#                                         success_score=5
#                                     )
#                                     feedback_col2.success("Thank you for your feedback!")
#                                     st.rerun()
# 
#                             with feedback_col3:
#                                 if st.button("ðŸ‘Ž", help="This response needs improvement"):
#                                     st.session_state.feedback_given = True
#                                     st.session_state.show_feedback_form[message_key] = True
#                                     st.rerun()
# 
#                         # Show feedback form if negative feedback was given
#                         if st.session_state.get("show_feedback_form", False):
#                             with st.form(key=f"feedback_form_{message_key}"):
#                                 feedback_col2.error("I apologize for the poor response. Please tell me what was wrong or missing:")
#                                 improvement_feedback = st.text_area(
#                                     "Your feedback",
#                                     key=f"improve_{message_key}",
#                                     help="Please explain what was unclear, incorrect, or missing in my response"
#                                 )
# 
#                                 if st.form_submit_button("Submit Feedback"):
#                                     if improvement_feedback:
#                                         with st.spinner("Generating improved response based on your feedback..."):
#                                             # Store detailed feedback
#                                             st.session_state.memory_manager.store_long_term_memory(
#                                                 query=prompt,
#                                                 response=f"Feedback: {improvement_feedback}",
#                                                 topic="improvement_feedback",
#                                                 success_score=1
#                                             )
# 
#                                             try:
#                                                 model = genai.GenerativeModel('gemini-1.5-flash')
#                                                 improvement_prompt = f"""
#                                                 Original Query: {prompt}
#                                                 Original Response: {response_message}
#                                                 User Feedback: {improvement_feedback}
# 
#                                                 Please provide an improved response that:
#                                                 1. Directly addresses the issues mentioned in the user's feedback
#                                                 2. If user asks for bullet points, format the response in bullet points
#                                                 3. If user asks for shorter response, make it concise
#                                                 4. If user asks for more details, provide comprehensive information
#                                                 5. Adapt the response format and style based on the feedback
#                                                 6. Maintain accuracy while being more helpful
# 
#                                                 Format the response according to user's feedback preferences.
#                                                 """
# 
#                                                 improved_response = model.generate_content(improvement_prompt)
#                                                 improved_text = improved_response.text
# 
#                                                 # Hide the feedback form
#                                                 st.session_state.show_feedback_form[message_key] = False
# 
#                                                 # Display improved response
#                                                 st.success("Based on your feedback, here's an improved response:")
#                                                 st.markdown(improved_text)
# 
#                                                 # Add to conversation history
#                                                 st.session_state.messages.append({
#                                                     "role": "assistant",
#                                                     "content": improved_text,
#                                                     "query": prompt,
#                                                     "is_improvement": True
#                                                 })
# 
#                                                 st.rerun()
# 
#                                             except Exception as e:
#                                                 st.error(f"Sorry, I couldn't generate an improved response: {str(e)}")
#                                     else:
#                                         st.warning("Please provide feedback before submitting.")
# 
#                         # Store in chat history
#                         st.session_state.messages.append({
#                             "role": "assistant",
#                             "content": response_message,
#                             "query": prompt
#                         })
# 
#                         # Update memory systems
#                         st.session_state.memory_manager.add_to_short_term_memory(prompt, response_message)
#                         st.session_state.conversation_history.append((prompt, response_message))
# 
#                         # Store query-answer pair
#                         store_query_answer(prompt, response_message, pc, pinecone_index_name)
# 
#                         # Update user profile
#                         st.session_state.user_profile['interaction_count'] += 1
# 
#                         # Store episodic memory if session has enough interactions
#                         if len(st.session_state.conversation_history) >= 5:
#                             st.session_state.memory_manager.store_episodic_memory(
#                                 session_id=st.session_state.user_id,
#                                 interactions=[{'query': q, 'response': r} for q, r in st.session_state.conversation_history]
#                             )
#                     else:
#                         st.error("I couldn't generate a response. Please try rephrasing your question.")
#                 else:
#                     st.error("Please make sure you have uploaded some documents first.")
# 
# 
#

# Install ngrok
!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz
!tar -xvzf ngrok-v3-stable-linux-amd64.tgz
!chmod +x ngrok

!./ngrok config add-authtoken 2lGTrPfKaseITcmgdp5ZXnaCrl6_2ABHvnjSSZ9WpLFvV8a2u

from subprocess import Popen
import time
import requests

# Start Streamlit in the background
streamlit_proc = Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'])

# Start ngrok to tunnel port 8501
ngrok_proc = Popen(['./ngrok', 'http', '8501'])

# Wait for ngrok to start and get the public URL
time.sleep(5)  # Give ngrok time to initialize
try:
    response = requests.get('http://localhost:4040/api/tunnels')
    public_url = response.json()['tunnels'][0]['public_url']
    print(f"Streamlit app is accessible at: {public_url}")
except Exception as e:
    print("Error getting ngrok URL:", e)

