{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHgTeIG-RcVX",
        "outputId": "3516d65f-8974-4008-fc07-e705d41986ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langchain-pinecone\n",
            "  Downloading langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (3.8)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pinecone<8.0.0,>=6.0.0 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading pinecone-7.1.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain-pinecone)\n",
            "  Downloading langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.4.26)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.4.0)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.172.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.36.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.42.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain-pinecone) (1.86.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain-pinecone) (0.9.0)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.6.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (9.0.0)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.1.2)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_pinecone-0.2.8-py3-none-any.whl (22 kB)\n",
            "Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_openai-0.3.23-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-7.1.0-py3-none-any.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.9/517.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.36.0-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (25 kB)\n",
            "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=5dc8607ffaca66a48ebb0a0e2b66ca0c4b3a0cac7c1b93759651b35838d773e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, watchdog, rapidfuzz, python-magic, python-iso639, python-dotenv, python-docx, PyPDF2, pypdf, pinecone-plugin-interface, olefile, mypy-extensions, marshmallow, langdetect, httpx-sse, emoji, backoff, vcrpy, typing-inspect, syrupy, python-oxmsg, pytest-socket, pytest-benchmark, pytest-asyncio, pydeck, pinecone-plugin-assistant, pinecone-client, unstructured-client, pytest-recording, pytest-codspeed, pydantic-settings, pinecone, dataclasses-json, aiohttp-retry, unstructured, streamlit, langchain-tests, langchain-openai, langchain-pinecone, langchain-google-genai, langchain-community\n",
            "Successfully installed PyPDF2-3.0.1 aiohttp-retry-2.9.1 backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.14.1 filetype-1.2.0 httpx-sse-0.4.0 langchain-community-0.3.25 langchain-google-genai-2.0.10 langchain-openai-0.3.23 langchain-pinecone-0.2.8 langchain-tests-0.3.20 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.1.0 olefile-0.47 pinecone-7.1.0 pinecone-client-6.0.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 pydantic-settings-2.9.1 pydeck-0.9.1 pypdf-5.6.0 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-3.2.0 pytest-recording-0.13.4 pytest-socket-0.7.0 python-docx-1.2.0 python-dotenv-1.1.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 streamlit-1.45.1 syrupy-4.9.1 typing-inspect-0.9.0 unstructured-0.17.2 unstructured-client-0.36.0 vcrpy-7.0.0 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit langchain langchain-community langchain-google-genai langchain-pinecone pinecone-client google-generativeai PyPDF2 python-docx markdown beautifulsoup4 unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import Pinecone as PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import google.generativeai as genai\n",
        "import uuid\n",
        "import re\n",
        "\n",
        "# Streamlit app configuration\n",
        "st.set_page_config(page_title=\"Document Processing Pipeline\", layout=\"wide\")\n",
        "st.title(\"Document Processing Pipeline\")\n",
        "st.subheader(\"Upload and process documents (PDF, DOCX, TXT, HTML, Markdown)\")\n",
        "\n",
        "# Sidebar for configuration\n",
        "st.sidebar.header(\"Configuration\")\n",
        "chunk_size = st.sidebar.slider(\"Chunk Size (characters)\", 500, 2000, 1000, 100)\n",
        "chunk_overlap = st.sidebar.slider(\"Chunk Overlap (characters)\", 0, 500, 200, 50)\n",
        "chunking_strategy = st.sidebar.selectbox(\"Chunking Strategy\", [\"Semantic\", \"Sliding Window\", \"Dynamic\"])\n",
        "\n",
        "# Hardcoded API keys\n",
        "gemini_api_key = \"AIzaSyB3sZS0oKnuVSyjyZKxyvRjpKa7nIoKYGg\"\n",
        "pinecone_api_key = \"pcsk_2samF1_Gu8hGM5YMPrZqqmGvADSYUvVE47EYJFFBJz5w5mZ3QinynS2Q7yagqcTPDQphtS\"\n",
        "pinecone_index_name = \"document-processing1\"\n",
        "\n",
        "# Initialize session state for user ID\n",
        "if \"user_id\" not in st.session_state:\n",
        "    st.session_state.user_id = str(uuid.uuid4())\n",
        "\n",
        "# Initialize chat history in session state\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Initialize feedback states\n",
        "if 'feedback_states' not in st.session_state:\n",
        "    st.session_state.feedback_states = {}\n",
        "\n",
        "# Initialize Gemini embeddings\n",
        "try:\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=gemini_api_key)\n",
        "except Exception as e:\n",
        "    embeddings_model = None\n",
        "    st.error(f\"Error initializing Gemini embeddings: {str(e)}\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "try:\n",
        "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
        "    pc = Pinecone(api_key=pinecone_api_key)\n",
        "    if pinecone_index_name not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=pinecone_index_name,\n",
        "            dimension=768,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "    vector_store = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings_model, namespace=\"chunks\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Error initializing Pinecone: {str(e)}\")\n",
        "    vector_store = None\n",
        "\n",
        "# Memory Management System\n",
        "class MemoryManager:\n",
        "    def __init__(self, pc, index_name):\n",
        "        self.pc = pc\n",
        "        self.index_name = index_name\n",
        "        self.short_term_memory = []  # Last 20 exchanges\n",
        "        self.max_short_term_size = 20\n",
        "        self.conversation_context = {\n",
        "            'topics': {},\n",
        "            'current_context': '',\n",
        "            'last_exchange': None\n",
        "        }\n",
        "\n",
        "    def add_to_short_term_memory(self, query, response):\n",
        "        # Create a new exchange\n",
        "        exchange = {\n",
        "            'query': query,\n",
        "            'response': response,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'topics': self._extract_topics(query, response)\n",
        "        }\n",
        "\n",
        "        # Update short term memory\n",
        "        self.short_term_memory.append(exchange)\n",
        "        if len(self.short_term_memory) > self.max_short_term_size:\n",
        "            self.short_term_memory.pop(0)\n",
        "\n",
        "        # Update conversation context\n",
        "        self._update_conversation_context(exchange)\n",
        "\n",
        "    def _extract_topics(self, query, response):\n",
        "        try:\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "            prompt = f\"\"\"Extract 3-5 key topics from this conversation exchange.\n",
        "            Format: Return ONLY a comma-separated list of topics, no other text.\n",
        "\n",
        "            User: {query}\n",
        "            Assistant: {response}\"\"\"\n",
        "\n",
        "            response = model.generate_content(prompt)\n",
        "            # Clean and validate the response\n",
        "            topics_text = response.text.strip()\n",
        "            if topics_text.startswith('['):\n",
        "                topics_text = topics_text[1:]\n",
        "            if topics_text.endswith(']'):\n",
        "                topics_text = topics_text[:-1]\n",
        "            topics = [topic.strip().strip('\"\\'') for topic in topics_text.split(',')]\n",
        "            return topics\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error extracting topics: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _update_conversation_context(self, exchange):\n",
        "        try:\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # Prepare the current context summary\n",
        "            current_context = self.conversation_context['current_context']\n",
        "\n",
        "            prompt = f\"\"\"Analyze this exchange and update the conversation context.\n",
        "            Current conversation context: {current_context}\n",
        "\n",
        "            New exchange:\n",
        "            User: {exchange['query']}\n",
        "            Assistant: {exchange['response']}\n",
        "\n",
        "            Instructions:\n",
        "            1. Identify key topics discussed\n",
        "            2. Note any references to previous topics\n",
        "            3. Summarize the current state of the conversation\n",
        "\n",
        "            Format your response in exactly this structure (keep the exact keys):\n",
        "            {{\n",
        "                \"topics_discussed\": [\"topic1\", \"topic2\"],\n",
        "                \"references_to_previous\": [\"reference1\", \"reference2\"],\n",
        "                \"conversation_summary\": \"A brief summary of the current state\"\n",
        "            }}\"\"\"\n",
        "\n",
        "            response = model.generate_content(prompt)\n",
        "\n",
        "            # Ensure proper JSON formatting\n",
        "            try:\n",
        "                # Clean the response text to ensure it's valid JSON\n",
        "                response_text = response.text.strip()\n",
        "                if response_text.startswith('```json'):\n",
        "                    response_text = response_text[7:]\n",
        "                if response_text.endswith('```'):\n",
        "                    response_text = response_text[:-3]\n",
        "                response_text = response_text.strip()\n",
        "\n",
        "                context_update = json.loads(response_text)\n",
        "\n",
        "                # Update topics\n",
        "                for topic in context_update.get('topics_discussed', []):\n",
        "                    if topic not in self.conversation_context['topics']:\n",
        "                        self.conversation_context['topics'][topic] = {\n",
        "                            'first_mentioned': len(self.short_term_memory) - 1,\n",
        "                            'mentions': [],\n",
        "                            'latest_context': ''\n",
        "                        }\n",
        "                    self.conversation_context['topics'][topic]['mentions'].append(len(self.short_term_memory) - 1)\n",
        "                    self.conversation_context['topics'][topic]['latest_context'] = context_update['conversation_summary']\n",
        "\n",
        "                # Update current context\n",
        "                self.conversation_context['current_context'] = context_update['conversation_summary']\n",
        "                self.conversation_context['last_exchange'] = exchange\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                st.error(f\"Error parsing context update: {str(e)}\")\n",
        "                # Fallback: Store basic context\n",
        "                self.conversation_context['current_context'] = f\"Last query: {exchange['query']}\"\n",
        "                self.conversation_context['last_exchange'] = exchange\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error updating conversation context: {str(e)}\")\n",
        "\n",
        "    def get_relevant_context(self, query, include_last_n=3):\n",
        "        try:\n",
        "            # Get recent context\n",
        "            recent_context = self.short_term_memory[-include_last_n:] if self.short_term_memory else []\n",
        "\n",
        "            # Get current topics from the query\n",
        "            query_topics = self._extract_topics(query, \"\")\n",
        "\n",
        "            # Build context information\n",
        "            context = {\n",
        "                'recent_exchanges': recent_context,\n",
        "                'current_context': self.conversation_context['current_context'],\n",
        "                'related_topics': {},\n",
        "                'conversation_summary': []\n",
        "            }\n",
        "\n",
        "            # Add topic-specific context\n",
        "            for topic in query_topics:\n",
        "                if topic in self.conversation_context['topics']:\n",
        "                    topic_data = self.conversation_context['topics'][topic]\n",
        "                    context['related_topics'][topic] = {\n",
        "                        'latest_context': topic_data['latest_context'],\n",
        "                        'related_exchanges': [\n",
        "                            self.short_term_memory[idx]\n",
        "                            for idx in topic_data['mentions'][-3:]\n",
        "                            if idx < len(self.short_term_memory)\n",
        "                        ]\n",
        "                    }\n",
        "\n",
        "            # Add conversation summaries\n",
        "            if recent_context:\n",
        "                context['conversation_summary'] = [\n",
        "                    f\"User: {ex['query']}\\nAssistant: {ex['response']}\"\n",
        "                    for ex in recent_context\n",
        "                ]\n",
        "\n",
        "            return context\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error getting relevant context: {str(e)}\")\n",
        "            return {\n",
        "                'recent_exchanges': [],\n",
        "                'current_context': '',\n",
        "                'related_topics': {},\n",
        "                'conversation_summary': []\n",
        "            }\n",
        "\n",
        "    def store_long_term_memory(self, query, response, topic, success_score):\n",
        "        try:\n",
        "            if not self.pc:\n",
        "                return\n",
        "\n",
        "            memory_id = f\"ltm_{uuid.uuid4()}\"\n",
        "            memory_embedding = embeddings_model.embed_query(f\"{query} {response}\")\n",
        "\n",
        "            self.pc.Index(self.index_name).upsert(\n",
        "                vectors=[{\n",
        "                    'id': memory_id,\n",
        "                    'values': memory_embedding,\n",
        "                    'metadata': {\n",
        "                        'query': query,\n",
        "                        'response': response,\n",
        "                        'topic': topic,\n",
        "                        'success_score': success_score,\n",
        "                        'timestamp': datetime.now().isoformat(),\n",
        "                        'type': 'long_term_memory'\n",
        "                    }\n",
        "                }],\n",
        "                namespace=\"memories\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error storing long-term memory: {str(e)}\")\n",
        "\n",
        "    def store_episodic_memory(self, session_id, interactions):\n",
        "        try:\n",
        "            if not self.pc:\n",
        "                return\n",
        "\n",
        "            episode_id = f\"episode_{session_id}_{datetime.now().isoformat()}\"\n",
        "            episode_summary = self._generate_episode_summary(interactions)\n",
        "            episode_embedding = embeddings_model.embed_query(episode_summary)\n",
        "\n",
        "            self.pc.Index(self.index_name).upsert(\n",
        "                vectors=[{\n",
        "                    'id': episode_id,\n",
        "                    'values': episode_embedding,\n",
        "                    'metadata': {\n",
        "                        'session_id': session_id,\n",
        "                        'summary': episode_summary,\n",
        "                        'interactions': interactions,\n",
        "                        'timestamp': datetime.now().isoformat(),\n",
        "                        'type': 'episodic_memory'\n",
        "                    }\n",
        "                }],\n",
        "                namespace=\"memories\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error storing episodic memory: {str(e)}\")\n",
        "\n",
        "    def _generate_episode_summary(self, interactions):\n",
        "        try:\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "            interactions_text = \"\\n\".join([f\"Q: {i['query']}\\nA: {i['response']}\" for i in interactions])\n",
        "            prompt = f\"\"\"Summarize this conversation session concisely:\n",
        "\n",
        "            {interactions_text}\n",
        "\n",
        "            Generate a brief summary that captures the main topics and outcomes.\"\"\"\n",
        "\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error generating episode summary: {str(e)}\")\n",
        "            return \"Error generating summary\"\n",
        "\n",
        "    def get_relevant_memories(self, query, top_k=3):\n",
        "        try:\n",
        "            if not self.pc:\n",
        "                return []\n",
        "\n",
        "            query_embedding = embeddings_model.embed_query(query)\n",
        "            results = self.pc.Index(self.index_name).query(\n",
        "                namespace=\"memories\",\n",
        "                vector=query_embedding,\n",
        "                top_k=top_k,\n",
        "                include_metadata=True\n",
        "            )\n",
        "\n",
        "            return [match['metadata'] for match in results['matches']]\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error retrieving memories: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# Initialize memory manager\n",
        "if 'memory_manager' not in st.session_state:\n",
        "    st.session_state.memory_manager = MemoryManager(pc, pinecone_index_name)\n",
        "\n",
        "# Initialize session state for conversation history\n",
        "if 'conversation_history' not in st.session_state:\n",
        "    st.session_state.conversation_history = []\n",
        "\n",
        "# Function to load document based on file type\n",
        "def load_document(file_path, file_type):\n",
        "    try:\n",
        "        if file_type == \"pdf\":\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif file_type == \"docx\":\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        elif file_type == \"txt\":\n",
        "            loader = TextLoader(file_path)\n",
        "        elif file_type == \"html\":\n",
        "            loader = UnstructuredHTMLLoader(file_path)\n",
        "        elif file_type == \"md\":\n",
        "            loader = UnstructuredMarkdownLoader(file_path)\n",
        "        else:\n",
        "            st.error(\"Unsupported file type\")\n",
        "            return None\n",
        "        return loader.load()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading document: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Semantic chunking based on paragraph boundaries\n",
        "def semantic_chunking(docs, chunk_size, chunk_overlap):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        "    )\n",
        "    return text_splitter.split_documents(docs)\n",
        "\n",
        "# Sliding window chunking\n",
        "def sliding_window_chunking(docs, chunk_size, chunk_overlap):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return text_splitter.split_documents(docs)\n",
        "\n",
        "# Dynamic chunk sizing based on content type\n",
        "def dynamic_chunking(docs, file_type):\n",
        "    if file_type == \"pdf\":\n",
        "        chunk_size, chunk_overlap = 800, 150\n",
        "    elif file_type == \"txt\":\n",
        "        chunk_size, chunk_overlap = 1200, 300\n",
        "    elif file_type == \"docx\":\n",
        "        chunk_size, chunk_overlap = 1000, 200\n",
        "    elif file_type == \"html\" or file_type == \"md\":\n",
        "        chunk_size, chunk_overlap = 600, 100\n",
        "    else:\n",
        "        chunk_size, chunk_overlap = 1000, 200\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return text_splitter.split_documents(docs)\n",
        "\n",
        "# Section detection (simplified: based on headings or double newlines)\n",
        "def detect_sections(docs):\n",
        "    sections = []\n",
        "    current_section = \"\"\n",
        "    for doc in docs:\n",
        "        content = doc.page_content\n",
        "        lines = content.split(\"\\n\")\n",
        "        for line in lines:\n",
        "            if line.startswith((\"#\", \"##\", \"###\")) or re.match(r\"<h[1-6]>\", line):\n",
        "                if current_section:\n",
        "                    sections.append(current_section.strip())\n",
        "                    current_section = \"\"\n",
        "            current_section += line + \"\\n\"\n",
        "        if current_section:\n",
        "            sections.append(current_section.strip())\n",
        "            current_section = \"\"\n",
        "    return sections if sections else [doc.page_content for doc in docs]\n",
        "\n",
        "# Generate hierarchical embeddings\n",
        "def generate_embeddings(docs, sections, chunks, file_name, file_type):\n",
        "    embeddings_data = {\"document\": {}, \"sections\": [], \"chunks\": []}\n",
        "    timestamp = datetime.now().isoformat()\n",
        "\n",
        "    # Document-level embedding\n",
        "    if embeddings_model:\n",
        "        try:\n",
        "            full_text = \" \".join([doc.page_content for doc in docs])\n",
        "            doc_embedding = embeddings_model.embed_query(full_text[:10000])  # Limit text length for document level\n",
        "            embeddings_data[\"document\"] = {\n",
        "                \"embedding\": doc_embedding,\n",
        "                \"metadata\": {\n",
        "                    \"source\": file_name,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"type\": \"document\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error generating document embedding: {str(e)}\")\n",
        "\n",
        "    # Section-level embeddings\n",
        "    if embeddings_model:\n",
        "        for i, section in enumerate(sections):\n",
        "            try:\n",
        "                # Limit section text length\n",
        "                section_text = section[:5000]\n",
        "                section_embedding = embeddings_model.embed_query(section_text)\n",
        "                embeddings_data[\"sections\"].append({\n",
        "                    \"embedding\": section_embedding,\n",
        "                    \"metadata\": {\n",
        "                        \"source\": file_name,\n",
        "                        \"section_id\": i + 1,\n",
        "                        \"timestamp\": timestamp,\n",
        "                        \"type\": \"section\",\n",
        "                        \"content\": section  # Store full section content\n",
        "                    }\n",
        "                })\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error generating section {i+1} embedding: {str(e)}\")\n",
        "\n",
        "    # Chunk-level embeddings with progress tracking\n",
        "    if embeddings_model:\n",
        "        with st.spinner('Generating chunk embeddings...'):\n",
        "            progress_bar = st.progress(0)\n",
        "            total_chunks = len(chunks)\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                try:\n",
        "                    # Update progress\n",
        "                    progress = int((i + 1) / total_chunks * 100)\n",
        "                    progress_bar.progress(progress)\n",
        "\n",
        "                    # Generate embedding for chunk\n",
        "                    chunk_text = chunk.page_content\n",
        "                    if chunk_text.strip():  # Only process non-empty chunks\n",
        "                        chunk_embedding = embeddings_model.embed_query(chunk_text)\n",
        "                        metadata = {\n",
        "                            \"source\": file_name,\n",
        "                            \"chunk_id\": i + 1,\n",
        "                            \"timestamp\": timestamp,\n",
        "                            \"type\": \"chunk\",\n",
        "                            \"content\": chunk_text\n",
        "                        }\n",
        "                        if file_type == \"pdf\" and hasattr(chunk, \"metadata\") and \"page\" in chunk.metadata:\n",
        "                            metadata[\"page_number\"] = chunk.metadata[\"page\"]\n",
        "\n",
        "                        embeddings_data[\"chunks\"].append({\n",
        "                            \"embedding\": chunk_embedding,\n",
        "                            \"metadata\": metadata\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error generating chunk {i+1} embedding: {str(e)}\")\n",
        "\n",
        "            progress_bar.empty()  # Remove progress bar when done\n",
        "\n",
        "    # Verify embeddings were generated\n",
        "    if not embeddings_data[\"chunks\"]:\n",
        "        st.warning(\"No chunk embeddings were generated. Please check your document content.\")\n",
        "    else:\n",
        "        st.success(f\"Successfully generated {len(embeddings_data['chunks'])} chunk embeddings\")\n",
        "\n",
        "    return embeddings_data\n",
        "\n",
        "# Store embeddings in Pinecone\n",
        "def store_in_pinecone(embeddings_data, pc, index_name):\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "        doc_id = f\"doc_{embeddings_data['document']['metadata']['source']}_{embeddings_data['document']['metadata']['timestamp']}\"\n",
        "        index.upsert(\n",
        "            vectors=[{\n",
        "                \"id\": doc_id,\n",
        "                \"values\": embeddings_data[\"document\"][\"embedding\"],\n",
        "                \"metadata\": embeddings_data[\"document\"][\"metadata\"]\n",
        "            }],\n",
        "            namespace=\"chunks\"\n",
        "        )\n",
        "        for section in embeddings_data[\"sections\"]:\n",
        "            section_id = f\"section_{section['metadata']['source']}_{section['metadata']['section_id']}_{section['metadata']['timestamp']}\"\n",
        "            index.upsert(\n",
        "                vectors=[{\n",
        "                    \"id\": section_id,\n",
        "                    \"values\": section[\"embedding\"],\n",
        "                    \"metadata\": section[\"metadata\"]\n",
        "                }],\n",
        "                namespace=\"chunks\"\n",
        "            )\n",
        "        for chunk in embeddings_data[\"chunks\"]:\n",
        "            chunk_id = f\"chunk_{chunk['metadata']['source']}_{chunk['metadata']['chunk_id']}_{chunk['metadata']['timestamp']}\"\n",
        "            index.upsert(\n",
        "                vectors=[{\n",
        "                    \"id\": chunk_id,\n",
        "                    \"values\": chunk[\"embedding\"],\n",
        "                    \"metadata\": chunk[\"metadata\"]\n",
        "                }],\n",
        "                namespace=\"chunks\"\n",
        "            )\n",
        "        st.success(\"Embeddings stored in Pinecone successfully!\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error storing embeddings in Pinecone: {str(e)}\")\n",
        "\n",
        "# Query Pinecone\n",
        "def query_pinecone(query, pc, index_name, top_k=3):\n",
        "    if not embeddings_model or not pc:\n",
        "        return None\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "        query_embedding = embeddings_model.embed_query(query)\n",
        "        results = index.query(\n",
        "            namespace=\"chunks\",\n",
        "            vector=query_embedding,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error querying Pinecone: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to get response from Gemini\n",
        "def get_gemini_response(query, retrieved_content):\n",
        "    try:\n",
        "        # Initialize the Gemini model\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "        # Get the last query expansion details if available\n",
        "        last_expansion = st.session_state.memory_manager.conversation_context.get('last_expansion', {})\n",
        "        expansion_context = \"\"\n",
        "        if last_expansion:\n",
        "            expansion_context = f\"\"\"\n",
        "Original query: {last_expansion.get('original_query', '')}\n",
        "Expanded to: {last_expansion.get('expanded_query', '')}\n",
        "Based on context: {chr(10).join(last_expansion.get('context_used', []))}\n",
        "\"\"\"\n",
        "\n",
        "        # Prepare the prompt with query context and retrieved content\n",
        "        prompt = f\"\"\"Generate a response to the user's query using the provided context.\n",
        "\n",
        "Query Context:\n",
        "{expansion_context}\n",
        "\n",
        "Current Query: {query}\n",
        "\n",
        "Retrieved Content:\n",
        "{retrieved_content}\n",
        "\n",
        "Instructions:\n",
        "1. Answer the query using information from the retrieved content\n",
        "2. Maintain context from the conversation\n",
        "3. If referring to previous topics, be explicit about what you're referring to\n",
        "4. If the answer requires information from previous exchanges, include that context\n",
        "5. Be clear and concise while being comprehensive\n",
        "\n",
        "Generate a natural, conversational response that directly answers the query.\"\"\"\n",
        "\n",
        "        # Generate response\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating response from Gemini: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Store interaction in Pinecone\n",
        "def store_interaction(query, pc, index_name):\n",
        "    if not embeddings_model or not pc:\n",
        "        return\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "        query_embedding = embeddings_model.embed_query(query)\n",
        "        interaction_id = f\"interaction_{st.session_state.user_id}_{datetime.now().isoformat()}\"\n",
        "        index.upsert(\n",
        "            vectors=[{\n",
        "                \"id\": interaction_id,\n",
        "                \"values\": query_embedding,\n",
        "                \"metadata\": {\n",
        "                    \"user_id\": st.session_state.user_id,\n",
        "                    \"query\": query,\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"type\": \"interaction\"\n",
        "                }\n",
        "            }],\n",
        "            namespace=\"interactions\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error storing interaction: {str(e)}\")\n",
        "\n",
        "# Store feedback in Pinecone\n",
        "def store_feedback(query_id, feedback, rating, pc, index_name):\n",
        "    if not pc:\n",
        "        return\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "        feedback_id = f\"feedback_{st.session_state.user_id}_{datetime.now().isoformat()}\"\n",
        "        index.upsert(\n",
        "            vectors=[{\n",
        "                \"id\": feedback_id,\n",
        "                \"values\": [0.0] * 768,\n",
        "                \"metadata\": {\n",
        "                    \"user_id\": st.session_state.user_id,\n",
        "                    \"query_id\": query_id,\n",
        "                    \"feedback\": feedback,\n",
        "                    \"rating\": rating,\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"type\": \"feedback\"\n",
        "                }\n",
        "            }],\n",
        "            namespace=\"feedback\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error storing feedback: {str(e)}\")\n",
        "\n",
        "# Store query-answer pair in Pinecone\n",
        "def store_query_answer(query, answer, pc, index_name):\n",
        "    if not embeddings_model or not pc:\n",
        "        return\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "        query_embedding = embeddings_model.embed_query(query)\n",
        "        qa_id = f\"qa_{st.session_state.user_id}_{datetime.now().isoformat()}\"\n",
        "        index.upsert(\n",
        "            vectors=[{\n",
        "                \"id\": qa_id,\n",
        "                \"values\": query_embedding,\n",
        "                \"metadata\": {\n",
        "                    \"query\": query,\n",
        "                    \"answer\": answer,\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"type\": \"query_answer\"\n",
        "                }\n",
        "            }],\n",
        "            namespace=\"query_answer_pairs\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error storing query-answer pair: {str(e)}\")\n",
        "\n",
        "# Query expansion and reformulation\n",
        "def expand_query(query, conversation_history, memory_manager):\n",
        "    try:\n",
        "        # Get relevant context from memory manager\n",
        "        context = memory_manager.get_relevant_context(query)\n",
        "\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "        # Build context sections\n",
        "        context_sections = []\n",
        "\n",
        "        # Add current conversation context\n",
        "        if context['current_context']:\n",
        "            context_sections.append(f\"Current conversation context:\\n{context['current_context']}\")\n",
        "\n",
        "        # Add recent conversation summary\n",
        "        if context['conversation_summary']:\n",
        "            context_sections.append(\"Recent conversation:\\n\" + \"\\n\".join(context['conversation_summary']))\n",
        "\n",
        "        # Add topic-specific context\n",
        "        if context['related_topics']:\n",
        "            topic_sections = []\n",
        "            for topic, topic_data in context['related_topics'].items():\n",
        "                topic_context = f\"Topic: {topic}\\n\"\n",
        "                topic_context += f\"Context: {topic_data['latest_context']}\\n\"\n",
        "                if topic_data['related_exchanges']:\n",
        "                    topic_context += \"Related exchanges:\\n\"\n",
        "                    for exchange in topic_data['related_exchanges']:\n",
        "                        topic_context += f\"Q: {exchange['query']}\\nA: {exchange['response']}\\n\"\n",
        "                topic_sections.append(topic_context)\n",
        "            context_sections.append(\"Topic-specific context:\\n\" + \"\\n\".join(topic_sections))\n",
        "\n",
        "        prompt = f\"\"\"Given the conversation context and current query, create an expanded search query.\n",
        "\n",
        "Context:\n",
        "{chr(10).join(context_sections)}\n",
        "\n",
        "Current Query: {query}\n",
        "\n",
        "Instructions:\n",
        "1. Analyze how the query relates to the conversation context\n",
        "2. Identify any references to previous topics or information\n",
        "3. Resolve pronouns and implicit references\n",
        "4. Include relevant context from previous exchanges\n",
        "5. Maintain the original intent of the question\n",
        "\n",
        "Return ONLY the expanded query text, no explanations.\"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        expanded_query = response.text.strip()\n",
        "\n",
        "        # Log the expansion for debugging\n",
        "        st.session_state.memory_manager.conversation_context['last_expansion'] = {\n",
        "            'original_query': query,\n",
        "            'expanded_query': expanded_query,\n",
        "            'context_used': context_sections\n",
        "        }\n",
        "\n",
        "        return expanded_query\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error expanding query: {str(e)}\")\n",
        "        return query\n",
        "\n",
        "def reformulate_query(query, user_profile):\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        prompt = f\"\"\"Given the user's profile and preferences, reformulate the query to be more precise:\n",
        "\n",
        "        User Profile:\n",
        "        {json.dumps(user_profile, indent=2)}\n",
        "\n",
        "        Original Query: {query}\n",
        "\n",
        "        Generate a reformulated query that considers the user's preferences and past interactions.\"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        reformulated_query = response.text\n",
        "        return reformulated_query\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reformulating query: {str(e)}\")\n",
        "        return query\n",
        "\n",
        "# Enhanced query processing with hybrid search\n",
        "def hybrid_search(query, pc, index_name, top_k=5):\n",
        "    if not embeddings_model or not pc:\n",
        "        return None\n",
        "    try:\n",
        "        # Semantic search\n",
        "        semantic_results = query_pinecone(query, pc, index_name, top_k=top_k)\n",
        "\n",
        "        # Keyword search using regex patterns\n",
        "        keyword_results = []\n",
        "        keywords = query.lower().split()\n",
        "        index = pc.Index(index_name)\n",
        "\n",
        "        # Fetch recent vectors\n",
        "        recent_vectors = index.query(\n",
        "            namespace=\"chunks\",\n",
        "            vector=[0.0] * 768,  # Dummy vector to fetch recent vectors\n",
        "            top_k=100,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        for vector in recent_vectors.matches:\n",
        "            content = vector.metadata.get('content', '').lower()\n",
        "            keyword_score = sum(1 for keyword in keywords if keyword in content)\n",
        "            if keyword_score > 0:\n",
        "                keyword_results.append({\n",
        "                    'id': vector.id,\n",
        "                    'score': keyword_score / len(keywords),\n",
        "                    'metadata': vector.metadata\n",
        "                })\n",
        "\n",
        "        # Combine and re-rank results\n",
        "        combined_results = []\n",
        "        seen_ids = set()\n",
        "\n",
        "        # Add semantic results\n",
        "        if semantic_results and 'matches' in semantic_results:\n",
        "            for match in semantic_results['matches']:\n",
        "                combined_results.append({\n",
        "                    'id': match['id'],\n",
        "                    'score': match['score'] * 0.7,  # Weight for semantic score\n",
        "                    'metadata': match['metadata']\n",
        "                })\n",
        "                seen_ids.add(match['id'])\n",
        "\n",
        "        # Add keyword results\n",
        "        for result in keyword_results:\n",
        "            if result['id'] not in seen_ids:\n",
        "                combined_results.append({\n",
        "                    'id': result['id'],\n",
        "                    'score': result['score'] * 0.3,  # Weight for keyword score\n",
        "                    'metadata': result['metadata']\n",
        "                })\n",
        "\n",
        "        # Sort by combined score\n",
        "        combined_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return {'matches': combined_results[:top_k]}\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in hybrid search: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# File uploader for document upload\n",
        "uploaded_file = st.file_uploader(\"Upload a document (PDF, DOCX, TXT, HTML, Markdown)\", type=[\"pdf\", \"docx\", \"txt\", \"html\", \"md\"])\n",
        "\n",
        "# Main processing logic\n",
        "if uploaded_file:\n",
        "    file_name = uploaded_file.name\n",
        "    file_type = file_name.split(\".\")[-1].lower()\n",
        "    file_path = f\"temp.{file_type}\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "    docs = load_document(file_path, file_type)\n",
        "    if docs:\n",
        "        st.success(\"Document loaded successfully!\")\n",
        "        sections = detect_sections(docs)\n",
        "        st.subheader(\"Detected Sections\")\n",
        "        st.write(f\"Total sections: {len(sections)}\")\n",
        "        for i, section in enumerate(sections[:3]):\n",
        "            with st.expander(f\"Section {i+1}\"):\n",
        "                st.write(section[:500] + \"...\" if len(section) > 500 else section)\n",
        "        if chunking_strategy == \"Semantic\":\n",
        "            chunks = semantic_chunking(docs, chunk_size, chunk_overlap)\n",
        "        elif chunking_strategy == \"Sliding Window\":\n",
        "            chunks = sliding_window_chunking(docs, chunk_size, chunk_overlap)\n",
        "        elif chunking_strategy == \"Dynamic\":\n",
        "            chunks = dynamic_chunking(docs, file_type)\n",
        "        st.subheader(\"Processed Chunks\")\n",
        "        st.write(f\"Total chunks: {len(chunks)}\")\n",
        "        for i, chunk in enumerate(chunks[:3]):\n",
        "            with st.expander(f\"Chunk {i+1}\"):\n",
        "                st.write(chunk.page_content)\n",
        "        if embeddings_model and vector_store:\n",
        "            embeddings_data = generate_embeddings(docs, sections, chunks, file_name, file_type)\n",
        "            store_in_pinecone(embeddings_data, pc, pinecone_index_name)\n",
        "            st.write(f\"Document-level embedding stored: {len(embeddings_data['document'].get('embedding', []))} dimensions\")\n",
        "            st.write(f\"Section-level embeddings stored: {len(embeddings_data['sections'])}\")\n",
        "            st.write(f\"Chunk-level embeddings stored: {len(embeddings_data['chunks'])}\")\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "\n",
        "# Chat interface\n",
        "st.subheader(\"Document Q&A Chatbot\")\n",
        "\n",
        "# Display chat messages\n",
        "for idx, message in enumerate(st.session_state.messages):\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            # Create feedback buttons\n",
        "            col1, col2, col3 = st.columns([1, 8, 1])\n",
        "\n",
        "            with col1:\n",
        "                if st.button(\"👍\", key=f\"like_{idx}\"):\n",
        "                    st.success(\"Thank you for your feedback!\")\n",
        "                    st.rerun()\n",
        "\n",
        "            with col3:\n",
        "                if st.button(\"👎\", key=f\"dislike_{idx}\"):\n",
        "                    # Show feedback form\n",
        "                    feedback = st.text_area(\"Please tell us what was wrong or missing:\", key=f\"feedback_{idx}\")\n",
        "                    if st.button(\"Submit Feedback\", key=f\"submit_{idx}\"):\n",
        "                        if feedback:\n",
        "                            # Generate improved response\n",
        "                            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "                            improvement_prompt = f\"\"\"\n",
        "                            Original response: {message['content']}\n",
        "                            User feedback: {feedback}\n",
        "\n",
        "                            Please provide an improved response that addresses the feedback.\n",
        "                            If they asked for bullet points, use bullet points.\n",
        "                            If they asked for a shorter response, be more concise.\n",
        "                            If they asked for more details, be more comprehensive.\n",
        "                            \"\"\"\n",
        "\n",
        "                            improved_response = model.generate_content(improvement_prompt)\n",
        "                            improved_text = improved_response.text\n",
        "\n",
        "                            # Add improved response to chat\n",
        "                            st.session_state.messages.append({\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"content\": improved_text,\n",
        "                                \"is_improvement\": True\n",
        "                            })\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.warning(\"Please provide feedback before submitting.\")\n",
        "\n",
        "# Chat input\n",
        "if prompt := st.chat_input(\"Ask me anything about your documents\"):\n",
        "    # Display user message\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Display assistant response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        if vector_store and embeddings_model:\n",
        "            # Get user profile from session state or initialize\n",
        "            if 'user_profile' not in st.session_state:\n",
        "                st.session_state.user_profile = {\n",
        "                    'session_id': st.session_state.user_id,\n",
        "                    'preferences': {},\n",
        "                    'interaction_count': 0\n",
        "                }\n",
        "\n",
        "            # Process query with context\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                # Expand and reformulate query using conversation history\n",
        "                expanded_query = expand_query(prompt, st.session_state.conversation_history, st.session_state.memory_manager)\n",
        "                reformulated_query = reformulate_query(expanded_query, st.session_state.user_profile)\n",
        "\n",
        "                # Get relevant memories\n",
        "                relevant_memories = st.session_state.memory_manager.get_relevant_memories(reformulated_query)\n",
        "\n",
        "                # Perform hybrid search\n",
        "                search_results = hybrid_search(reformulated_query, pc, pinecone_index_name)\n",
        "\n",
        "                if search_results and \"matches\" in search_results:\n",
        "                    retrieved_contents = []\n",
        "                    context_sections = []\n",
        "\n",
        "                    # Format relevant memories\n",
        "                    if relevant_memories:\n",
        "                        memory_context = \"Previous relevant interactions:\\n\"\n",
        "                        for memory in relevant_memories:\n",
        "                            memory_context += f\"Q: {memory.get('query', '')}\\nA: {memory.get('response', '')}\\n\"\n",
        "                        context_sections.append(memory_context)\n",
        "\n",
        "                    # Format search results\n",
        "                    for match in search_results[\"matches\"]:\n",
        "                        content = match['metadata'].get('content', '')\n",
        "                        if content and content != 'N/A':\n",
        "                            source = match['metadata'].get('source', 'Unknown')\n",
        "                            page = match['metadata'].get('page_number', '')\n",
        "                            page_info = f\" (Page {page})\" if page else \"\"\n",
        "                            context_sections.append(f\"From {source}{page_info}:\\n{content}\")\n",
        "                            retrieved_contents.append(content)\n",
        "\n",
        "                    # Combine all context\n",
        "                    retrieved_text = \"\\n\\n\".join(context_sections)\n",
        "\n",
        "                    if not retrieved_text:\n",
        "                        retrieved_text = \"No relevant content found.\"\n",
        "\n",
        "                    # Get response from Gemini\n",
        "                    response_message = get_gemini_response(prompt, retrieved_text)\n",
        "                    if response_message:\n",
        "                        # Display the response\n",
        "                        st.write(response_message)\n",
        "\n",
        "                        # Immediate feedback UI\n",
        "                        feedback_col1, feedback_col2, feedback_col3 = st.columns([1, 8, 1])\n",
        "\n",
        "                        # Show feedback buttons if feedback hasn't been given\n",
        "                        if \"feedback_given\" not in st.session_state:\n",
        "                            st.session_state.feedback_given = False\n",
        "\n",
        "                        if not st.session_state.feedback_given:\n",
        "                            with feedback_col1:\n",
        "                                if st.button(\"👍\", help=\"This response was helpful\"):\n",
        "                                    st.session_state.feedback_given = True\n",
        "                                    st.session_state.memory_manager.store_long_term_memory(\n",
        "                                        query=prompt,\n",
        "                                        response=response_message,\n",
        "                                        topic=\"general\",\n",
        "                                        success_score=5\n",
        "                                    )\n",
        "                                    feedback_col2.success(\"Thank you for your feedback!\")\n",
        "                                    st.rerun()\n",
        "\n",
        "                            with feedback_col3:\n",
        "                                if st.button(\"👎\", help=\"This response needs improvement\"):\n",
        "                                    st.session_state.feedback_given = True\n",
        "                                    st.session_state.show_feedback_form[message_key] = True\n",
        "                                    st.rerun()\n",
        "\n",
        "                        # Show feedback form if negative feedback was given\n",
        "                        if st.session_state.get(\"show_feedback_form\", False):\n",
        "                            with st.form(key=f\"feedback_form_{message_key}\"):\n",
        "                                feedback_col2.error(\"I apologize for the poor response. Please tell me what was wrong or missing:\")\n",
        "                                improvement_feedback = st.text_area(\n",
        "                                    \"Your feedback\",\n",
        "                                    key=f\"improve_{message_key}\",\n",
        "                                    help=\"Please explain what was unclear, incorrect, or missing in my response\"\n",
        "                                )\n",
        "\n",
        "                                if st.form_submit_button(\"Submit Feedback\"):\n",
        "                                    if improvement_feedback:\n",
        "                                        with st.spinner(\"Generating improved response based on your feedback...\"):\n",
        "                                            # Store detailed feedback\n",
        "                                            st.session_state.memory_manager.store_long_term_memory(\n",
        "                                                query=prompt,\n",
        "                                                response=f\"Feedback: {improvement_feedback}\",\n",
        "                                                topic=\"improvement_feedback\",\n",
        "                                                success_score=1\n",
        "                                            )\n",
        "\n",
        "                                            try:\n",
        "                                                model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "                                                improvement_prompt = f\"\"\"\n",
        "                                                Original Query: {prompt}\n",
        "                                                Original Response: {response_message}\n",
        "                                                User Feedback: {improvement_feedback}\n",
        "\n",
        "                                                Please provide an improved response that:\n",
        "                                                1. Directly addresses the issues mentioned in the user's feedback\n",
        "                                                2. If user asks for bullet points, format the response in bullet points\n",
        "                                                3. If user asks for shorter response, make it concise\n",
        "                                                4. If user asks for more details, provide comprehensive information\n",
        "                                                5. Adapt the response format and style based on the feedback\n",
        "                                                6. Maintain accuracy while being more helpful\n",
        "\n",
        "                                                Format the response according to user's feedback preferences.\n",
        "                                                \"\"\"\n",
        "\n",
        "                                                improved_response = model.generate_content(improvement_prompt)\n",
        "                                                improved_text = improved_response.text\n",
        "\n",
        "                                                # Hide the feedback form\n",
        "                                                st.session_state.show_feedback_form[message_key] = False\n",
        "\n",
        "                                                # Display improved response\n",
        "                                                st.success(\"Based on your feedback, here's an improved response:\")\n",
        "                                                st.markdown(improved_text)\n",
        "\n",
        "                                                # Add to conversation history\n",
        "                                                st.session_state.messages.append({\n",
        "                                                    \"role\": \"assistant\",\n",
        "                                                    \"content\": improved_text,\n",
        "                                                    \"query\": prompt,\n",
        "                                                    \"is_improvement\": True\n",
        "                                                })\n",
        "\n",
        "                                                st.rerun()\n",
        "\n",
        "                                            except Exception as e:\n",
        "                                                st.error(f\"Sorry, I couldn't generate an improved response: {str(e)}\")\n",
        "                                    else:\n",
        "                                        st.warning(\"Please provide feedback before submitting.\")\n",
        "\n",
        "                        # Store in chat history\n",
        "                        st.session_state.messages.append({\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": response_message,\n",
        "                            \"query\": prompt\n",
        "                        })\n",
        "\n",
        "                        # Update memory systems\n",
        "                        st.session_state.memory_manager.add_to_short_term_memory(prompt, response_message)\n",
        "                        st.session_state.conversation_history.append((prompt, response_message))\n",
        "\n",
        "                        # Store query-answer pair\n",
        "                        store_query_answer(prompt, response_message, pc, pinecone_index_name)\n",
        "\n",
        "                        # Update user profile\n",
        "                        st.session_state.user_profile['interaction_count'] += 1\n",
        "\n",
        "                        # Store episodic memory if session has enough interactions\n",
        "                        if len(st.session_state.conversation_history) >= 5:\n",
        "                            st.session_state.memory_manager.store_episodic_memory(\n",
        "                                session_id=st.session_state.user_id,\n",
        "                                interactions=[{'query': q, 'response': r} for q, r in st.session_state.conversation_history]\n",
        "                            )\n",
        "                    else:\n",
        "                        st.error(\"I couldn't generate a response. Please try rephrasing your question.\")\n",
        "                else:\n",
        "                    st.error(\"Please make sure you have uploaded some documents first.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkieuyyoSIUg",
        "outputId": "cda51dce-20eb-4fdb-d087-ffe94144e42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "!chmod +x ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RodupORISn7d",
        "outputId": "75277a45-598a-4b20-9beb-0abd933395b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-17 18:04:07--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 35.71.179.82, 13.248.244.96, 75.2.60.68, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|35.71.179.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9543082 (9.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz.37’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]   9.10M  10.7MB/s    in 0.9s    \n",
            "\n",
            "2025-06-17 18:04:09 (10.7 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz.37’ saved [9543082/9543082]\n",
            "\n",
            "ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok config add-authtoken 2lGTrPfKaseITcmgdp5ZXnaCrl6_2ABHvnjSSZ9WpLFvV8a2u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FSMXxOcSmb1",
        "outputId": "14eac669-a101-45e7-98b6-3e4c7ee73d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from subprocess import Popen\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start Streamlit in the background\n",
        "streamlit_proc = Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n",
        "\n",
        "# Start ngrok to tunnel port 8501\n",
        "ngrok_proc = Popen(['./ngrok', 'http', '8501'])\n",
        "\n",
        "# Wait for ngrok to start and get the public URL\n",
        "time.sleep(5)  # Give ngrok time to initialize\n",
        "try:\n",
        "    response = requests.get('http://localhost:4040/api/tunnels')\n",
        "    public_url = response.json()['tunnels'][0]['public_url']\n",
        "    print(f\"Streamlit app is accessible at: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(\"Error getting ngrok URL:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsG6XYgySs39",
        "outputId": "78672b73-efaa-441c-c623-7e4ec321e6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is accessible at: https://5d72-34-16-190-164.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyQL04WqSub1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}